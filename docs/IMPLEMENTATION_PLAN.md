# VideoLingo Quality Improvements - Implementation Plan

> –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞
> –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã: P0 (must do) ‚Üí P1 (next) ‚Üí P2 (nice to have)

---

## P0.1 ‚Äî Overlap + –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —á–∞–Ω–∫–æ–≤ ‚úÖ DONE

> **PR:** https://github.com/carden-code/VideoLingo/pull/22

### –ü—Ä–æ–±–ª–µ–º–∞
–°–µ–π—á–∞—Å –≤ `audio_preprocess.py:split_audio()` —á–∞–Ω–∫–∏ —Ä–µ–∂—É—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –±–µ–∑ overlap.
–ü—Ä–∏ —Å–∫–ª–µ–π–∫–µ –≤ `_2_asr.py` —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ—Å—Ç–æ `.extend()` –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–µ–π.

### –†–µ—à–µ–Ω–∏–µ

**–§–∞–π–ª:** `core/asr_backend/audio_preprocess.py`

```python
def split_audio(audio_file: str, target_len: float = 30*60, win: float = 60,
                overlap: float = 1.5) -> List[Tuple[float, float]]:
    """
    –†–∞–∑—Ä–µ–∑–∞–µ—Ç –∞—É–¥–∏–æ –Ω–∞ —á–∞–Ω–∫–∏ —Å overlap –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö.

    Args:
        overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (default 1.5s)

    Returns:
        List of (start, end) tuples with overlapping regions
    """
    audio = AudioSegment.from_file(audio_file)
    duration = float(mediainfo(audio_file)["duration"])

    if duration <= target_len + win:
        return [(0, duration)]

    segments, pos = [], 0.0
    safe_margin = 0.5

    while pos < duration:
        if duration - pos <= target_len:
            segments.append((pos, duration))
            break

        threshold = pos + target_len
        ws, we = int((threshold - win) * 1000), int((threshold + win) * 1000)

        silence_regions = detect_silence(audio[ws:we], min_silence_len=500, silence_thresh=-30)
        silence_regions = [(s/1000 + (threshold - win), e/1000 + (threshold - win))
                          for s, e in silence_regions]

        valid_regions = [
            (start, end) for start, end in silence_regions
            if (end - start) >= (safe_margin * 2) and threshold <= start + safe_margin <= threshold + win
        ]

        if valid_regions:
            split_at = valid_regions[0][0] + safe_margin
        else:
            split_at = threshold

        # –î–æ–±–∞–≤–ª—è–µ–º overlap –∫ –∫–æ–Ω—Ü—É —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞ (–ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Å –Ω–∞—á–∞–ª–æ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ)
        chunk_end = min(split_at + overlap, duration)
        segments.append((pos, chunk_end))

        # –°–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ë–ï–ó overlap (overlap —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ)
        pos = split_at

    rprint(f"[green]üéôÔ∏è Audio split: {len(segments)} segments with {overlap}s overlap[/green]")
    return segments
```

**–§–∞–π–ª:** `core/_2_asr.py` ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é

```python
from core.utils import *
from core.asr_backend.demucs_vl import demucs_audio
from core.asr_backend.audio_preprocess import (
    process_transcription, convert_video_to_audio, split_audio,
    save_results, normalize_audio_volume, deduplicate_segments
)
from core._1_ytdlp import find_video_files
from core.utils.models import *

@check_file_exists(_2_CLEANED_CHUNKS)
def transcribe():
    video_file = find_video_files()
    convert_video_to_audio(video_file)

    if load_key("demucs"):
        demucs_audio()
        vocal_audio = normalize_audio_volume(_VOCAL_AUDIO_FILE, _VOCAL_AUDIO_FILE, format="mp3")
    else:
        vocal_audio = _RAW_AUDIO_FILE

    # –ü–æ–ª—É—á–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Å overlap
    segments = split_audio(_RAW_AUDIO_FILE, overlap=1.5)

    from core.asr_backend.whisperX_local import transcribe_audio as ts

    all_results = []
    for start, end in segments:
        result = ts(_RAW_AUDIO_FILE, vocal_audio, start, end)
        all_results.append((start, end, result))

    # –ù–û–í–û–ï: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —É—á—ë—Ç–æ–º overlap
    combined_result = deduplicate_segments(all_results, segments, overlap=1.5)

    df = process_transcription(combined_result)
    save_results(df)
```

**–§–∞–π–ª:** `core/asr_backend/audio_preprocess.py` ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏

```python
def deduplicate_segments(all_results: List[Tuple[float, float, Dict]],
                         segments: List[Tuple[float, float]],
                         overlap: float = 1.5,
                         tolerance: float = 0.05) -> Dict:
    """
    –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å–ª–æ–≤ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö overlap-—á–∞–Ω–∫–æ–≤.

    –õ–æ–≥–∏–∫–∞: –µ—Å–ª–∏ —Å–ª–æ–≤–æ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ overlap-–∑–æ–Ω—É –ò —É–∂–µ –µ—Å—Ç—å –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —á–∞–Ω–∫–µ
    (–ø–æ –≤—Ä–µ–º–µ–Ω–∏ start/end), —Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ.
    """
    combined = {'segments': []}
    drop_before = None  # –î–æ –∫–∞–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–∫–æ–Ω–µ—Ü overlap)

    for i, (chunk_start, chunk_end, result) in enumerate(all_results):
        if i > 0:
            prev_end = segments[i - 1][1]
            drop_before = max(chunk_start, prev_end - tolerance)

        for segment in result['segments']:
            new_words = []
            for word in segment.get('words', []):
                word_start = word.get('start')
                word_end = word.get('end', word_start)

                if word_start is None:
                    word_start = segment.get('start')  # if still None -> mark segment suspect

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –ø–æ–∫—Ä—ã—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏–º —á–∞–Ω–∫–æ–º
                if word_start is not None and drop_before is not None:
                    if word_start < (drop_before - tolerance):
                        continue

                new_words.append(word)

            if new_words:
                new_segment = segment.copy()
                new_segment['words'] = new_words
                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º start/end —Å–µ–≥–º–µ–Ω—Ç–∞
                new_segment['start'] = new_words[0].get('start', segment['start'])
                new_segment['end'] = new_words[-1].get('end', segment['end'])
                combined['segments'].append(new_segment)

    rprint(f"[cyan]üîó Deduplicated: {sum(len(s.get('words', [])) for s in combined['segments'])} words[/cyan]")
    return combined
```

**Acceptance criteria (P0.1):**
- –ù–∞ —Å—Ç—ã–∫–∞—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è 1‚Äì3 —Å–ª–æ–≤–∞.
- –ù–µ—Ç –æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ (–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–∞–π–ª–µ >30 –º–∏–Ω).

---

## P0.2 ‚Äî –ó–∞–º–µ–Ω–∏—Ç—å substring-matching –Ω–∞ word-index spans ‚úÖ DONE

> **PR:** https://github.com/carden-code/VideoLingo/pull/23

### –ü—Ä–æ–±–ª–µ–º–∞
–í `_6_gen_sub.py:get_sentence_timestamps()` —Ç–∞–π–º–∫–æ–¥—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ–ª—É—á–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ substring matching –≤ "—Å–ª–µ–ø–ª–µ–Ω–Ω–æ–º" –ø–æ—Ç–æ–∫–µ —Å–ª–æ–≤. –≠—Ç–æ –ª–æ–º–∞–µ—Ç—Å—è –ø—Ä–∏ –ª—é–±–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏/–ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏.

### –†–µ—à–µ–Ω–∏–µ: Word-Index Spans (—Å–ø–∞–Ω—ã —Ä–æ–∂–¥–∞—é—Ç—Å—è –≤ split)

–ö–ª—é—á–µ–≤–æ–µ –ø—Ä–∞–≤–∏–ª–æ: **–Ω–∏–∫–∞–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–ø–∞–Ω–æ–≤ —á–µ—Ä–µ–∑ substring/SequenceMatcher/difflib**.
–°—Ç–∞–¥–∏–∏ split (`_3_1_split_nlp.py` / `_3_2_split_meaning.py`) —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ word timeline
–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç `segment_id + word_start_idx/word_end_idx + text`.
–¢–∞–π–º–∏–Ω–≥–∏ –±–µ—Ä—É—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º —Å–ª–æ–≤.

**–ù–æ–≤—ã–π —Ñ–∞–π–ª:** `core/utils/segment_index.py`

```python
"""
Segment Index - —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ word timestamps.

–ö–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç —Ö—Ä–∞–Ω–∏—Ç –¥–∏–∞–ø–∞–∑–æ–Ω –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ–≤ [word_start_idx, word_end_idx],
–∞ –Ω–µ —Ç–µ–∫—Å—Ç –¥–ª—è substring matching.
"""

import pandas as pd
from dataclasses import dataclass
from typing import List, Optional, Tuple
import uuid
from rich import print as rprint


@dataclass
class Segment:
    """–ê—Ç–æ–º–∞—Ä–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ word timeline."""
    id: str
    text: str
    word_start_idx: int  # –ò–Ω–¥–µ–∫—Å –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –≤ cleaned_chunks
    word_end_idx: int    # –ò–Ω–¥–µ–∫—Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞ (inclusive)
    start_time: float = 0.0
    end_time: float = 0.0
    speaker_id: Optional[str] = None
    translation: Optional[str] = None

    # ASR confidence (P1.1)
    avg_logprob: Optional[float] = None
    no_speech_prob: Optional[float] = None

    @classmethod
    def generate_id(cls) -> str:
        return str(uuid.uuid4())[:8]


class SegmentIndex:
    """
    –ò–Ω–¥–µ–∫—Å —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ word-level timestamps.

    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "substring matching" ‚Äî —Ç–µ–ø–µ—Ä—å —Å–≤—è–∑—å —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤.
    """

    def __init__(self, words_df: pd.DataFrame):
        """
        Args:
            words_df: DataFrame –∏–∑ cleaned_chunks.xlsx —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
                      text, start, end, speaker_id
        """
        self.words_df = words_df.reset_index(drop=True)
        self.words_df['word_idx'] = self.words_df.index
        self.segments: List[Segment] = []

        # word_idx only; spans are produced during split stage

    def create_segment_from_span(self,
                                 segment_id: str,
                                 text: str,
                                 word_start_idx: int,
                                 word_end_idx: int,
                                 speaker_id: Optional[str] = None) -> Segment:
        """
        –°–æ–∑–¥–∞—ë—Ç —Å–µ–≥–º–µ–Ω—Ç –Ω–∞–ø—Ä—è–º—É—é –∏–∑ word-span.
        """
        start_time = float(self.words_df.loc[word_start_idx, 'start'])
        end_time = float(self.words_df.loc[word_end_idx, 'end'])
        if speaker_id is None:
            speaker_id = self.words_df.loc[word_start_idx, 'speaker_id']

        segment = Segment(
            id=segment_id,
            text=text,
            word_start_idx=word_start_idx,
            word_end_idx=word_end_idx,
            start_time=start_time,
            end_time=end_time,
            speaker_id=speaker_id
        )

        self.segments.append(segment)
        return segment

    def build_from_sentences(self, segments: List[dict]) -> List[Segment]:
        """
        –°–æ–∑–¥–∞—ë—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä —Å –≥–æ—Ç–æ–≤—ã–º–∏ word-span.

        Args:
            segments: [{segment_id, text, word_start_idx, word_end_idx, speaker_id?}]

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏ —Ç–∞–π–º–∫–æ–¥–∞–º–∏
        """
        result = []
        for item in segments:
            seg = self.create_segment_from_span(
                segment_id=item['segment_id'],
                text=item['text'],
                word_start_idx=item['word_start_idx'],
                word_end_idx=item['word_end_idx'],
                speaker_id=item.get('speaker_id')
            )
            result.append(seg)

        return result

    def to_dataframe(self) -> pd.DataFrame:
        """Export segments to DataFrame."""
        return pd.DataFrame([
            {
                'segment_id': s.id,
                'text': s.text,
                'word_start_idx': s.word_start_idx,
                'word_end_idx': s.word_end_idx,
                'start_time': s.start_time,
                'end_time': s.end_time,
                'duration': s.end_time - s.start_time,
                'speaker_id': s.speaker_id,
                'translation': s.translation,
            }
            for s in self.segments
        ])

    def update_segment_translation(self, segment_id: str, translation: str):
        """Update translation for a segment by ID."""
        for seg in self.segments:
            if seg.id == segment_id:
                seg.translation = translation
                return
        raise ValueError(f"Segment not found: {segment_id}")

    def merge_segments(self, seg1_id: str, seg2_id: str) -> Segment:
        """
        Merge two adjacent segments.

        Returns new merged segment, removes originals from index.
        """
        seg1 = next((s for s in self.segments if s.id == seg1_id), None)
        seg2 = next((s for s in self.segments if s.id == seg2_id), None)

        if not seg1 or not seg2:
            raise ValueError("Segment not found")

        if seg1.word_end_idx + 1 != seg2.word_start_idx:
            rprint(f"[yellow]‚ö†Ô∏è Merging non-adjacent segments[/yellow]")

        merged = Segment(
            id=Segment.generate_id(),
            text=seg1.text + ' ' + seg2.text,
            word_start_idx=seg1.word_start_idx,
            word_end_idx=seg2.word_end_idx,
            start_time=seg1.start_time,
            end_time=seg2.end_time,
            speaker_id=seg1.speaker_id,
            translation=(seg1.translation or '') + ' ' + (seg2.translation or '')
                        if seg1.translation or seg2.translation else None
        )

        # Remove old, add new
        self.segments = [s for s in self.segments if s.id not in (seg1_id, seg2_id)]
        self.segments.append(merged)
        self.segments.sort(key=lambda s: s.word_start_idx)

        return merged


def load_segment_index(words_file: str = "output/log/cleaned_chunks.xlsx") -> SegmentIndex:
    """Load words and create segment index."""
    df = pd.read_excel(words_file)
    df['text'] = df['text'].str.strip('"').str.strip()
    return SegmentIndex(df)
```

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: split-—Å—Ç–∞–¥–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ word timeline, –∞ –Ω–µ –ø–æ –≥–æ–ª–æ–º—É —Ç–µ–∫—Å—Ç—É.
–î–ª—è CJK –∏ —è–∑—ã–∫–æ–≤ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å—Ç—Ä–æ–∏—Ç—Å—è –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º —Å–ª–æ–≤/—Å–∏–º–≤–æ–ª–æ–≤,
–∞ –Ω–µ —á–µ—Ä–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∞–Ω–æ–≤ –ø–æ —Å—Ç—Ä–æ–∫–∞–º.

–ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ split-—Å—Ç–∞–¥–∏–∏:
```python
words_df = pd.read_excel(_2_CLEANED_CHUNKS)
tokens = words_df['text'].tolist()
# split_tokens(...) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (word_start_idx, word_end_idx)
spans = split_tokens(tokens)
segments = [
    {
        'segment_id': f'seg_{i:04d}',
        'word_start_idx': start,
        'word_end_idx': end,
        'text': ' '.join(tokens[start:end + 1]),
    }
    for i, (start, end) in enumerate(spans)
]
pd.DataFrame(segments).to_excel(_3_2_SEGMENTS, index=False)
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ `_6_gen_sub.py`

```python
# –ó–∞–º–µ–Ω–∏—Ç—å get_sentence_timestamps() –Ω–∞:

from core.utils.segment_index import load_segment_index, SegmentIndex

def align_timestamp_v2(df_translate: pd.DataFrame, output_dir: str):
    """
    –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è align_timestamp —á–µ—Ä–µ–∑ SegmentIndex.
    """
    # Load word-level timestamps
    index = load_segment_index()

    # Build segments from spans with stable IDs
    segments_df = pd.read_excel(_3_2_SEGMENTS)
    segments = index.build_from_sentences(segments_df.to_dict('records'))

    # Add translations by segment_id
    translations = dict(zip(df_translate['segment_id'], df_translate['Translation']))
    for seg in segments:
        if seg.id in translations:
            seg.translation = translations[seg.id]

    # Export
    df_result = index.to_dataframe()

    # Generate SRT files
    for filename, columns in SUBTITLE_OUTPUT_CONFIGS:
        generate_srt(df_result, columns, os.path.join(output_dir, filename))

    return df_result
```

**Acceptance criteria (P0.2):**
- 100% —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–º–µ—é—Ç `segment_id`, `word_start_idx`, `word_end_idx`.
- –¢–∞–π–º–∫–æ–¥—ã –±–µ—Ä—É—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –∏–∑ spans, –±–µ–∑ substring/SequenceMatcher.

---

## P0.3 ‚Äî –°—Ç–∞–±–∏–ª—å–Ω—ã–π segment_id —á–µ—Ä–µ–∑ –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω

### –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö —Ñ–∞–π–ª–æ–≤

**–ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç `cleaned_chunks.xlsx` (word-level, –±–µ–∑ segment_id):**
```
| word_idx | text    | start  | end    | speaker_id |
|----------|---------|--------|--------|------------|
| 0        | Hello   | 0.240  | 0.560  | SPEAKER_00 |
| 1        | world   | 0.580  | 0.920  | SPEAKER_00 |
```

**–ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç `segments.xlsx` (segment-level):**
```
| segment_id | parent_segment_id | source_chunk_id | text         | word_start_idx | word_end_idx | start  | end    | speaker_id |
|------------|-------------------|-----------------|--------------|----------------|--------------|--------|--------|------------|
| seg_0001   |                   | chunk_0001      | Hello world  | 0              | 1            | 0.240  | 0.920  | SPEAKER_00 |
```

**–ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç `translation_results.xlsx`:**
```
| segment_id | parent_segment_id | Source              | Translation          | start  | end    |
|------------|-------------------|---------------------|----------------------|--------|--------|
| seg_0001   |                   | Hello world         | –ü—Ä–∏–≤–µ—Ç –º–∏—Ä           | 0.240  | 0.920  |
| seg_0002   |                   | This is a test      | –≠—Ç–æ —Ç–µ—Å—Ç             | 1.100  | 2.300  |
```

**–ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç `tts_tasks.xlsx`:**
```
| segment_id | parent_segment_id | number | start_time | end_time | text         | origin      | est_dur |
|------------|-------------------|--------|------------|----------|--------------|-------------|---------|
| seg_0001   |                   | 1      | 00:00:00.2 | 00:00:00.9| –ü—Ä–∏–≤–µ—Ç –º–∏—Ä   | Hello world | 0.8     |
```

**–ü—Ä–∞–≤–∏–ª–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤:**
- `segment_id` —É–Ω–∏–∫–∞–ª–µ–Ω –∏ –Ω–µ–∏–∑–º–µ–Ω—è–µ–º.
- –õ—é–±–æ–π split/merge —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π `segment_id`, –∞ –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `parent_segment_id`.
- `source_chunk_id` –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω, –Ω–æ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ ASR —á–∞–Ω–∫–∞.

**Acceptance criteria (P0.3):**
- –û—Ç –ª—é–±–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ `tts_tasks.xlsx` –º–æ–∂–Ω–æ –ø—Ä–æ–π—Ç–∏ –¥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ ASR —á–∞–Ω–∫–∞ –ø–æ id.

### –ü—Ä–æ–±—Ä–æ—Å segment_id

```python
# –í _3_2_split_meaning.py - –ø–æ—Å–ª–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ split —Å–æ–∑–¥–∞—ë–º segment_id
def split_by_meaning():
    # ... existing split logic ...
    final_sentences = load_final_sentences()

    segments_with_ids = []
    for i, sentence in enumerate(final_sentences):
        segments_with_ids.append({
            'segment_id': f'seg_{i:04d}',
            'text': sentence
        })

    # Save with IDs (segment-level table)
    pd.DataFrame(segments_with_ids).to_excel(_3_2_SEGMENTS, index=False)

# –í–∞–∂–Ω–æ: –ø—Ä–∏ split/merge –≤ _5_split_sub.py —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–µ segment_id,
# –∞ —Å—Ç–∞—Ä—ã–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ parent_segment_id (–¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏).
# source_chunk_id –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—Å—è –æ—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤.
```

---

## P1.1 ‚Äî –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ASR confidence —Å–∏–≥–Ω–∞–ª–æ–≤

**–§–∞–π–ª:** `core/asr_backend/whisperX_local.py`

```python
def transcribe_audio(raw_audio_file, vocal_audio_file, start, end):
    # ... existing code ...

    result = model.transcribe(raw_audio_segment, batch_size=batch_size, print_progress=True)
    raw_transcribe = result  # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ align

    # ... alignment ...
    aligned = whisperx.align(...)

    # –ù–û–í–û–ï: –ø–µ—Ä–µ–Ω–æ—Å–∏–º confidence –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ aligned —Å–µ–≥–º–µ–Ω—Ç—ã
    for seg, raw_seg in zip(aligned['segments'], raw_transcribe['segments']):
        seg['_confidence'] = {
            'avg_logprob': raw_seg.get('avg_logprob', None),
            'no_speech_prob': raw_seg.get('no_speech_prob', None),
            'compression_ratio': raw_seg.get('compression_ratio', None),
            'temperature': raw_seg.get('temperature', None),
        }

    return aligned
```

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ align –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
–ø–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤, –∞ –Ω–µ –ø–æ zip.

**–§–∞–π–ª:** `core/asr_backend/audio_preprocess.py`

```python
def process_transcription(result: Dict) -> pd.DataFrame:
    all_words = []
    for segment in result['segments']:
        speaker_id = segment.get('speaker_id', None)
        confidence = segment.get('_confidence', {})

        for word in segment['words']:
            word_dict = {
                'text': word["word"],
                'start': word.get('start', ...),
                'end': word['end'],
                'speaker_id': speaker_id,
                # –ù–û–í–û–ï: Confidence –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–µ–≥–º–µ–Ω—Ç–∞
                'segment_avg_logprob': confidence.get('avg_logprob'),
                'segment_no_speech_prob': confidence.get('no_speech_prob'),
                # –§–ª–∞–≥ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ —Å–ª–æ–≤–∞
                'is_zero_duration': word.get('end', 0) <= word.get('start', 0),
            }
            all_words.append(word_dict)

    return pd.DataFrame(all_words)


def flag_suspicious_segments(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü–æ–º–µ—á–∞–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–ª–∏ re-ASR.

    –ö—Ä–∏—Ç–µ—Ä–∏–∏:
    - avg_logprob < -1.0 (–Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
    - no_speech_prob > 0.5 (–≤–µ—Ä–æ—è—Ç–Ω–æ —Ç–∏—à–∏–Ω–∞)
    - >20% zero-duration —Å–ª–æ–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
    """
    df['is_suspicious'] = False

    # Low confidence
    if 'segment_avg_logprob' in df.columns:
        df.loc[df['segment_avg_logprob'] < -1.0, 'is_suspicious'] = True

    # Likely silence
    if 'segment_no_speech_prob' in df.columns:
        df.loc[df['segment_no_speech_prob'] > 0.5, 'is_suspicious'] = True

    # Many zero-duration words
    if 'is_zero_duration' in df.columns:
        zero_pct = df['is_zero_duration'].mean()
        if zero_pct > 0.2:
            rprint(f"[yellow]‚ö†Ô∏è High zero-duration rate: {zero_pct:.1%}[/yellow]")

    suspicious_count = df['is_suspicious'].sum()
    if suspicious_count > 0:
        rprint(f"[yellow]‚ö†Ô∏è Flagged {suspicious_count} suspicious words for review[/yellow]")

    return df
```

---

## P1.2 ‚Äî Hard constraints –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤

**–§–∞–π–ª:** `core/_4_2_translate.py`

```python
def validate_anchors(source: str, translation: str, terms: List[dict]) -> dict:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–ª—é—á–µ–≤—ã–µ anchors —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å –≤ –ø–µ—Ä–µ–≤–æ–¥–µ.

    Args:
        source: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        translation: –ü–µ—Ä–µ–≤–æ–¥
        terms: –°–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ terminology.json

    Returns:
        {'valid': bool, 'missing': [...], 'wrong': [...]}
    """
    issues = {'valid': True, 'missing': [], 'wrong': []}

    for term in terms:
        src_term = term.get('src', '')
        tgt_term = term.get('tgt', '')
        note = term.get('note', '')

        # Check if source term appears in source text
        if src_term.lower() not in source.lower():
            continue

        # Case 1: "keep" - term should appear unchanged
        if 'keep' in note.lower() or '–Ω–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å' in note.lower():
            if src_term.lower() not in translation.lower():
                issues['missing'].append(src_term)
                issues['valid'] = False

        # Case 2: Specific translation required
        elif tgt_term and tgt_term != src_term:
            if tgt_term.lower() not in translation.lower():
                issues['wrong'].append({'expected': tgt_term, 'source': src_term})
                issues['valid'] = False

    # Check numbers preserved
    src_numbers = re.findall(r'\d+(?:\.\d+)?%?', source)
    for num in src_numbers:
        if num not in translation:
            issues['missing'].append(f"number: {num}")
            issues['valid'] = False

    return issues


def translate_with_anchor_validation(chunk, terms, max_retries=3):
    """
    –ü–µ—Ä–µ–≤–æ–¥ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π anchors.
    –ü—Ä–∏ –ø—Ä–æ–≤–∞–ª–µ - retry —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.
    """
    for attempt in range(max_retries):
        translation = translate_chunk_basic(chunk)

        anchor_check = validate_anchors(chunk, translation, terms)

        if anchor_check['valid']:
            return translation

        if attempt < max_retries - 1:
            rprint(f"[yellow]‚ö†Ô∏è Anchor validation failed: {anchor_check}[/yellow]")
            # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–µ
            # (–¥–æ–±–∞–≤–∏—Ç—å –≤ –ø—Ä–æ–º–ø—Ç —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã)

    raise ValueError(f"Anchor validation failed after {max_retries} attempts")
```

---

## P1.3 ‚Äî –≠—Å–∫–∞–ª–∞—Ü–∏—è retry –ø–µ—Ä–µ–≤–æ–¥–∞

**–§–∞–π–ª:** `core/translate_lines.py`

```python
def retry_translation_with_escalation(prompt_fn, lines, step_name, max_retries=3):
    """
    Retry —Å —ç—Å–∫–∞–ª–∞—Ü–∏–µ–π:
    1. –û–±—ã—á–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    2. –°—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç (literal, no paraphrase)
    3. –†–∞–∑–±–∏–µ–Ω–∏–µ —á–∞–Ω–∫–∞ –ø–æ–ø–æ–ª–∞–º
    """

    for attempt in range(max_retries):
        try:
            if attempt == 0:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                prompt = prompt_fn(lines, strict=False)
            elif attempt == 1:
                # –°—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç
                prompt = prompt_fn(lines, strict=True)
                console.print(f"[yellow]üîÑ Retry #{attempt+1}: strict mode[/yellow]")
            else:
                # –†–∞–∑–±–∏–µ–Ω–∏–µ —á–∞–Ω–∫–∞ –ø–æ–ø–æ–ª–∞–º
                mid = len(lines.split('\n')) // 2
                lines_list = lines.split('\n')
                first_half = '\n'.join(lines_list[:mid])
                second_half = '\n'.join(lines_list[mid:])

                console.print(f"[yellow]üîÑ Retry #{attempt+1}: splitting chunk[/yellow]")

                result1 = retry_translation_with_escalation(prompt_fn, first_half, step_name, 2)
                result2 = retry_translation_with_escalation(prompt_fn, second_half, step_name, 2)

                # Merge results
                return merge_translation_results(result1, result2)

            result = ask_gpt(prompt, resp_type='json', ...)

            if validate_result(result, lines):
                return result

        except Exception as e:
            if attempt == max_retries - 1:
                raise
            console.print(f"[yellow]‚ö†Ô∏è Attempt {attempt+1} failed: {e}[/yellow]")

    raise ValueError(f"Translation failed after {max_retries} attempts with escalation")
```

---

## P1.4 ‚Äî –ü–µ—Ä-—Å–µ–≥–º–µ–Ω—Ç–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–∞

### –ü—Ä–æ–±–ª–µ–º–∞
–°–µ–π—á–∞—Å duration-aware –ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–∞–Ω–∫–∞, –Ω–æ –Ω–µ –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞.
–ò–∑-–∑–∞ —ç—Ç–æ–≥–æ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã –º–æ–≥—É—Ç –≤—ã—Ö–æ–¥–∏—Ç—å –¥–ª–∏–Ω–Ω–µ–µ/–∫–æ—Ä–æ—á–µ –∏ ¬´–ø–æ–ª–∑—Ç–∏¬ª –ø–æ —Ç–∞–π–º–ª–∞–π–Ω—É.

### –†–µ—à–µ–Ω–∏–µ
–ü–µ—Ä–µ–¥–∞–≤–∞—Ç—å LLM –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å **–∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞** –∏ —Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥, —É–∫–ª–∞–¥—ã–≤–∞—é—â–∏–π—Å—è –≤ –µ—ë –æ–∫–Ω–æ.
–û–ø–∏—Ä–∞–µ–º—Å—è –Ω–∞ word-span —Ç–∞–π–º–∏–Ω–≥–∏ (P0.2).

**–ò–¥–µ—è:**
```python
for segment in segments:
    duration_info = {
        "total_duration": segment.end_time - segment.start_time,
        "src_chars": len(segment.text),
    }
    translation = translate_lines(segment.text, ..., duration_info=duration_info)
```

**Acceptance criteria (P1.4):**
- 80‚Äì90% —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —É–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –±–µ–∑ time-stretch.
- –ù–µ—Ç –º–∞—Å—Å–æ–≤—ã—Ö ¬´–ø–µ—Ä–µ–ª—ë—Ç–æ–≤¬ª –≤ —Å–æ—Å–µ–¥–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã.

---

## P2.1 ‚Äî Ducking –¥–ª—è —Ñ–æ–Ω–∞

**–§–∞–π–ª:** `core/_12_dub_to_vid.py`

```python
def merge_video_audio_with_ducking():
    """
    –§–∏–Ω–∞–ª—å–Ω—ã–π –º–∏–∫—Å —Å sidechain compression (ducking).
    –§–æ–Ω –ø—Ä–∏–≥–ª—É—à–∞–µ—Ç—Å—è –∫–æ–≥–¥–∞ –∏–≥—Ä–∞–µ—Ç —Ä–µ—á—å.
    """
    VIDEO_FILE = find_video_files()
    background_file = _BACKGROUND_AUDIO_FILE

    # Normalize dub audio
    normalized_dub_audio = 'output/normalized_dub.wav'
    normalize_audio_volume(DUB_AUDIO, normalized_dub_audio)

    # FFmpeg filter —Å sidechaincompress
    # –ö–æ–≥–¥–∞ dub –≥—Ä–æ–º—á–µ threshold ‚Äî background –ø—Ä–∏–≥–ª—É—à–∞–µ—Ç—Å—è
    audio_filter = '''
    [1:a]asplit=2[bg][sc];
    [2:a]asplit=2[dub][ducksig];
    [bg][ducksig]sidechaincompress=
        threshold=0.02:
        ratio=4:
        attack=50:
        release=300:
        makeup=1
    [bgducked];
    [bgducked][dub]amix=inputs=2:duration=first:weights=0.3 1[a]
    '''

    # Alternative: –ø—Ä–æ—Å—Ç–æ–π lowpass –Ω–∞ —Ñ–æ–Ω–µ –≤–æ –≤—Ä–µ–º—è —Ä–µ—á–∏
    # –ú–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ, –Ω–æ –ª–µ–≥—á–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å
    audio_filter_simple = '''
    [1:a][2:a]sidechaincompress=
        threshold=0.01:
        ratio=3:
        attack=20:
        release=200
    [compressed];
    [compressed][2:a]amix=inputs=2:duration=first[a]
    '''

    cmd = [
        'ffmpeg', '-y',
        '-i', VIDEO_FILE,
        '-i', background_file,
        '-i', normalized_dub_audio,
        '-filter_complex', audio_filter,
        '-map', '0:v',
        '-map', '[a]',
        '-c:a', 'aac', '-b:a', '128k',
        DUB_VIDEO
    ]

    subprocess.run(cmd)
```

---

## P2.2 ‚Äî –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–º–µ–¥–∏–∞—Ü–∏—è duration mismatch

**–§–∞–π–ª:** `core/_10_gen_audio.py`

```python
def remediate_duration_mismatch(segment_id: str,
                                 source_text: str,
                                 text: str,
                                 target_duration: float,
                                 actual_duration: float,
                                 attempt: int = 0) -> str:
    """
    –†–µ–º–µ–¥–∏–∞—Ü–∏—è –∫–æ–≥–¥–∞ TTS –¥–ª–∏–Ω–Ω–µ–µ —Ç–∞—Ä–≥–µ—Ç–∞.

    –ü–æ—Ä—è–¥–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π:
    1. –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –∫–æ—Ä–æ—á–µ (length-aware retranslate)
    2. Split —Å–µ–≥–º–µ–Ω—Ç–∞ –Ω–∞ 2
    3. Speedup (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö max)
    4. Trim ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫—Ä–∞–π
    """
    ratio = actual_duration / target_duration
    max_speed = load_key("speed_factor.max")

    if ratio <= max_speed:
        # –ú–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ —É—Å–∫–æ—Ä–∏—Ç—å
        return apply_speedup(segment_id, ratio)

    if attempt == 0:
        # –ü–æ–ø—ã—Ç–∫–∞ 1: –ü–æ–ø—Ä–æ—Å–∏—Ç—å LLM —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥
        console.print(f"[yellow]üìù Duration {ratio:.1f}x too long, requesting shorter translation[/yellow]")

        anchors = extract_anchors(source_text)
        shorter_text = request_shorter_translation(
            source_text,
            text,
            target_duration,
            actual_duration,
            anchors
        )

        if not validate_anchors(source_text, shorter_text, anchors):
            console.print("[yellow]‚ö†Ô∏è Anchor validation failed, splitting segment instead[/yellow]")
            return remediate_duration_mismatch(
                segment_id, source_text, text, target_duration, actual_duration, attempt + 1
            )

        # Regenerate TTS
        new_audio = regenerate_tts(segment_id, shorter_text)
        new_duration = get_audio_duration(new_audio)

        if new_duration / target_duration <= max_speed:
            return apply_speedup(segment_id, new_duration / target_duration)

        # Recurse with next strategy
        return remediate_duration_mismatch(
            segment_id, source_text, shorter_text, target_duration, new_duration, attempt + 1
        )

    elif attempt == 1:
        # –ü–æ–ø—ã—Ç–∫–∞ 2: Split —Å–µ–≥–º–µ–Ω—Ç–∞
        console.print(f"[yellow]‚úÇÔ∏è Splitting segment {segment_id}[/yellow]")
        # ... split logic ...

    else:
        # Last resort: trim
        if ratio <= max_speed * 1.1:  # 10% tolerance
            console.print(f"[yellow]‚ö†Ô∏è Trimming audio (last resort)[/yellow]")
            return trim_audio(segment_id, target_duration * max_speed)
        else:
            raise ValueError(f"Cannot remediate duration: {ratio:.1f}x too long")


def request_shorter_translation(source_text: str,
                                text: str,
                                target_dur: float,
                                actual_dur: float,
                                anchors: dict) -> str:
    """–ó–∞–ø—Ä–æ—Å –∫ LLM –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º anchors."""

    chars_to_remove = int(len(text) * (1 - target_dur / actual_dur))

    prompt = f"""
Shorten this translation to fit in {target_dur:.1f} seconds (currently takes {actual_dur:.1f}s).
Remove {chars_to_remove} characters while preserving meaning.
Do NOT change anchors (numbers, terms, acronyms, currencies).

Source: "{source_text}"
Text: "{text}"

Output JSON: {{"shortened": "shorter version here"}}
"""

    result = ask_gpt(prompt, resp_type='json', log_title='duration_fix')
    return result['shortened']
```

---

## P2.3 ‚Äî VAD –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Å—Ç–∞—Ä—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–æ–≤

### –ü—Ä–æ–±–ª–µ–º–∞
–†–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –¥–ª–∏–Ω—É –∏ –ø–∞—É–∑—ã, –∏–∑-–∑–∞ —á–µ–≥–æ —Å—Ç–∞—Ä—Ç —Ñ—Ä–∞–∑—ã –º–æ–∂–µ—Ç —É–µ–∑–∂–∞—Ç—å –æ—Ç –¥–≤–∏–∂–µ–Ω–∏—è –≥—É–±.

### –†–µ—à–µ–Ω–∏–µ
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å VAD (–Ω–∞–ø—Ä–∏–º–µ—Ä, Silero) –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤ —Ä–µ—á–∏ –≤ –∏—Å—Ö–æ–¥–Ω–∏–∫–µ
–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–µ–±–æ–ª—å—à–æ–≥–æ –æ–∫–Ω–∞.

**–ê–≤—Ç–æ-–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ (–±–µ–∑ —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥ —Ä–æ–ª–∏–∫):**
- –û—Ü–µ–Ω–∏—Ç—å —à—É–º–æ–≤–æ–π —Ñ–æ–Ω –Ω–∞ –ø–µ—Ä–≤—ã—Ö 30‚Äì60 —Å–µ–∫ –∏ –≤—ã—Å—Ç–∞–≤–∏—Ç—å –ø–æ—Ä–æ–≥ VAD –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–≥–æ.
- –ï—Å–ª–∏ VAD –¥–∞—ë—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—É—é ¬´—Ä–µ—á—å –±–µ–∑ –ø–∞—É–∑¬ª,
  –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–∏—Ç—å VAD –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ (fail-safe).
- –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Å–¥–≤–∏–≥ `max_shift_ms` (–Ω–∞–ø—Ä–∏–º–µ—Ä, 200‚Äì300 –º—Å), —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å —Ç–∞–π–º–ª–∞–π–Ω.

**–ü—Ä–µ—Å–µ—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):**
- `lecture` (–¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –º–µ–Ω—å—à–µ –ø–∞—É–∑)
- `interview` (—á–∞—Å—Ç—ã–µ —Å–º–µ–Ω—ã —Å–ø–∏–∫–µ—Ä–æ–≤)
- `noisy` (–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)

**–ò–¥–µ—è:**
```python
vad_segments = detect_speech_segments(audio_path)  # [(start, end), ...]
segment.start_time = snap_to_nearest_vad_onset(segment.start_time, vad_segments, max_shift=0.3)
```

**Acceptance criteria (P2.3):**
- –°—Ç–∞—Ä—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å VAD-onset (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 200‚Äì300 –º—Å).
- –ù–µ —É—Ö—É–¥—à–∞–µ—Ç—Å—è –æ–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–∞–π–º–ª–∞–π–Ω–∞.
- –ü—Ä–∏ –ø–ª–æ—Ö–æ–º –∫–∞—á–µ—Å—Ç–≤–µ VAD –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è (fail-safe).

---

## –ü–æ—Ä—è–¥–æ–∫ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è

### –ù–µ–¥–µ–ª—è 1: P0 (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ)
1. P0.1: Overlap + –¥–µ–¥—É–ø –≤ `audio_preprocess.py` –∏ `_2_asr.py`
2. P0.2: `SegmentIndex` class
3. P0.3: segment_id –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö —Ñ–∞–π–ª–æ–≤

### –ù–µ–¥–µ–ª—è 2: P1 (—É–∫—Ä–µ–ø–ª–µ–Ω–∏–µ)
4. P1.1: ASR confidence –≤ `cleaned_chunks.xlsx`
5. P1.2: Anchor validation
6. P1.3: Retry escalation
7. P1.4: –ü–µ—Ä-—Å–µ–≥–º–µ–Ω—Ç–Ω–æ–µ duration-aware –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ

### –ù–µ–¥–µ–ª—è 3: P2 (polish)
8. P2.1: Ducking
9. P2.2: Duration remediation
10. P2.3: VAD-aligned starts

---

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –¢–µ—Å—Ç –¥–ª—è P0.1 (overlap + –¥–µ–¥—É–ø)
```bash
# –ù–∞–π—Ç–∏ –≤–∏–¥–µ–æ >35 –º–∏–Ω—É—Ç
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö 30-–º–∏–Ω—É—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ—Ç:
# - –û–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
# - –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
# - Gaps –≤ —Ç–∞–π–º–ª–∞–π–Ω–µ
```

### –¢–µ—Å—Ç –¥–ª—è P0.2 (word-index spans)
```bash
# –ü–æ–¥–∞—Ç—å –∑–∞—Ä–∞–Ω–µ–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–π word timeline + split
# –í—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å word_start_idx/word_end_idx
# –¢–∞–π–º–∫–æ–¥—ã —Å—á–∏—Ç–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ span (–Ω–∏–∫–∞–∫–∏—Ö substring/SequenceMatcher)
```

### –¢–µ—Å—Ç –¥–ª—è P0.3 (segment_id)
```bash
# –ü–æ—Å–ª–µ merge/split –æ–ø–µ—Ä–∞—Ü–∏–π –ø—Ä–æ–≤–µ—Ä–∏—Ç—å:
# - –í—Å–µ segment_id —É–Ω–∏–∫–∞–ª—å–Ω—ã
# - parent_segment_id —Å–æ—Ö—Ä–∞–Ω—ë–Ω –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ split/merge
# - –°–≤—è–∑—å origin‚Üîtranslation –Ω–µ –ø–æ—Ç–µ—Ä—è–Ω–∞
# - tts_tasks —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
```

### –¢–µ—Å—Ç –¥–ª—è P1.4 (per-segment duration-aware)
```bash
# –í—ã–±—Ä–∞—Ç—å 20‚Äì30 —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω–æ–π
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å: –¥–æ–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏—Ö time-stretch > 80%
# –ù–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ—Ä–∞–∑–º–µ—Ä–∞ (ratio > 1.25)
```

### –¢–µ—Å—Ç –¥–ª—è P2.2 (duration remediation)
```bash
# –í—Ö–æ–¥: —Å–µ–≥–º–µ–Ω—Ç —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–º–µ—Ç–Ω–æ –¥–ª–∏–Ω–Ω–µ–µ —Ç–∞—Ä–≥–µ—Ç–∞
# –û–∂–∏–¥–∞–Ω–∏–µ: shorten/split —Å–Ω–∏–∂–∞–µ—Ç ratio < 1.5 –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç anchors (—á–∏—Å–ª–∞/—Ç–µ—Ä–º–∏–Ω—ã)
```

### –¢–µ—Å—Ç –¥–ª—è P2.3 (VAD alignment)
```bash
# –í—Ö–æ–¥: –≤–∏–¥–µ–æ —Å –∑–∞–º–µ—Ç–Ω—ã–º–∏ –ø–∞—É–∑–∞–º–∏ –≤ —Ä–µ—á–∏
# –û–∂–∏–¥–∞–Ω–∏–µ: —Å—Ç–∞—Ä—Ç —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å VAD-onset (¬±300ms)
# Fail-safe: –ø—Ä–∏ "—à—É–º–Ω–æ–º" VAD –æ–Ω –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è, —Ç–∞–π–º–ª–∞–π–Ω –æ—Å—Ç–∞—ë—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω—ã–º
```

---

*–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: –î–µ–∫–∞–±—Ä—å 2024*
